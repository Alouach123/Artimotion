// This is an autogenerated file from Firebase Studio.
'use server';
/**
 * @fileOverview AI agent that creates a short animation based on a character and background image.
 *
 * - createAnimation - A function that generates a 5-second animation based on the provided character and background images.
 * - CreateAnimationInput - The input type for the createAnimation function.
 * - CreateAnimationOutput - The return type for the createAnimation function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const CreateAnimationInputSchema = z.object({
  characterDataUri: z
    .string()
    .describe(
      "A photo of the character, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  backgroundDataUri: z
    .string()
    .describe(
      "A photo of the background, as a data URI that must include a MIME type and use Base64 encoding. Expected format: 'data:<mimetype>;base64,<encoded_data>'."
    ),
  sceneDescription: z.string().describe('A description of the scene and the desired interaction between the character and the background.'),
});
export type CreateAnimationInput = z.infer<typeof CreateAnimationInputSchema>;

const CreateAnimationOutputSchema = z.object({
  animationDataUri: z
    .string()
    .describe(
      'The generated animation as a data URI (e.g., GIF or video) that must include a MIME type and use Base64 encoding.'
    ),
});
export type CreateAnimationOutput = z.infer<typeof CreateAnimationOutputSchema>;

export async function createAnimation(input: CreateAnimationInput): Promise<CreateAnimationOutput> {
  return createAnimationFlow(input);
}

const animationPrompt = ai.definePrompt({
  name: 'animationPrompt',
  input: {schema: CreateAnimationInputSchema},
  output: {schema: CreateAnimationOutputSchema},
  prompt: `You are an AI that generates short animations based on a character image and a background image.

  The animation should be 5 seconds long and show the character interacting with the background in a way that is consistent with the scene description.

  Character Image: {{media url=characterDataUri}}
  Background Image: {{media url=backgroundDataUri}}
  Scene Description: {{{sceneDescription}}}

  Generate a short animation showing the character interacting with the background as described. Return the animation as a data URI.
  `,
});

const createAnimationFlow = ai.defineFlow(
  {
    name: 'createAnimationFlow',
    inputSchema: CreateAnimationInputSchema,
    outputSchema: CreateAnimationOutputSchema,
  },
  async input => {
    const {media} = await ai.generate({
      model: 'googleai/gemini-2.0-flash-exp',
      prompt: [
        {media: {url: input.characterDataUri}},
        {media: {url: input.backgroundDataUri}},
        {text: input.sceneDescription + 'Generate a 5 second animation of this scene.'},
      ],
      config: {
        responseModalities: ['TEXT', 'IMAGE'],
      },
    });

    return {animationDataUri: media.url!};
  }
);
